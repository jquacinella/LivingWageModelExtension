{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master's Thesis - James Quacinella\n",
    "\n",
    "\n",
    "# Abstract\n",
    "\n",
    "**Objectives:** This study will extend an established model for estimating the current living wage in 2015 to the past decade for the purpose of:\n",
    "\n",
    "* an exploratory analysis trends in the gap between the estimated living wage and the minimum wage\n",
    "* evaluating any correlation between the living wage gap and other economic metrics, including public funds spent on social services\n",
    "\n",
    "**Methods:** The original data set for this model is for 2015. This study will extend the data sources of this model into the past to enable trend analysis. Data for economic metrics from public data sources will supplement this data for correlation analysis.\n",
    "\n",
    "\n",
    "# Methods\n",
    "\n",
    "## Model\n",
    "\n",
    "The original model proposed estimated the living wage in terms of 9 variables:\n",
    "\n",
    "** *basic_needs_budget* ** = *food_cost* + *child_care_cost* + ( *insurance_premiums* + *health_care_costs* ) + *housing_cost* + *transportation_cost* + *other_necessities_cost*\n",
    "\n",
    "** *living_wage* ** = *basic_needs_budget* + ( *basic_needs_budget* \\* *tax_rate* )\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "The following data sources are used to find estimates of the model variables:\n",
    "\n",
    "* The food cost is estimated from data from the USDA’s low-cost food plan national average in June 2014.\n",
    "* Child care is based off state-level estimates published by the National Association of Child Care Resource and Referral Agencies.\n",
    "* Insurance costs are based on the insurance component of the 2013 Medical Expenditure Panel Survey.\n",
    "* Housing costs are estimated from the HUD Fair Market Rents (FMR) estimates\n",
    "* Other variables are pulled from the 2014 Bureau of Labor Statistics Consumer Expenditure Survey.\n",
    "\n",
    "These data sets extend into the past, allowing for calculating the model for years past. The data will also have to be adjusted for inflation 6.\n",
    "\n",
    "## Analytic Approach\n",
    "\n",
    "First, data will be gathered from the data sources of the original model but will be extended into the past. The methodology followed by the model will be replicated to come up with a data set representing estimates of the living wage across time. After the data set is prepared, the trend of the living wage as compared to minimum wage can be examined. Has the gap increased or decreased over time, and at what rate? Have certain areas seen larger than average increases or decreases in this gap? \n",
    "\n",
    "Once preliminary trend analysis is done, this data set will be analyzed in comparison to other economic trends to see if any interesting correlations can be found. Correlations to GDP growth rate and the national rate of unemployment can be made, but the primary investigation will be to see if the living wage gap correlates to national spending on SNAP (Food stamps). In other words, we will see if there is any (potentially time lagged) relationship between the living wage gap and how much the United States needs to spend to support those who cannot make ends meet. A relationship here can potentially indicate that shrinking this gap could lower public expenditures.\n",
    "\n",
    "\n",
    "## Presentation Of Results\n",
    "\n",
    "Results will be presented for both parts of the data analysis. For studying the living wage gap trends, this report will present graphs of time series, aggregated in different ways, of the living wage as well as the living wage gap. Some of these time series will be presented along side data on public expenditures on SNAP to visually inspect for correlations.\n",
    "\n",
    "## Background / Sources\n",
    "\n",
    "- Glasmeier AK, Nadeau CA, Schultheis E: LIVING WAGE CALCULATOR User’s Guide / Technical Notes 2014 Update\n",
    "- USDA low-cost food plan, June, 2014\n",
    "- Child Care in America 2014 State fact sheets\n",
    "- 2013 Medical Expenditure Panel Survey Available\n",
    "- Consumer Expenditure Survey\n",
    "- Inflation Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "------\n",
    "\n",
    "------\n",
    "\n",
    "# Pre-Data Collection\n",
    "\n",
    "Lets do all of our imports now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prettytable import PrettyTable\n",
    "from IPython.core.display import HTML\n",
    "from collections import OrderedDict, defaultdict\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# Path to local dir on my laptop\n",
    "PROJECT_PATH = \"/home/james/Development/Masters/Thesis\" # Path to project files on my local computer\n",
    "\n",
    "def constant_factory(value):\n",
    "    ''' Always prodcues a constant value; used fo defaultdict '''\n",
    "    return itertools.repeat(value).next\n",
    "\n",
    "def caption(msg, tablenum):\n",
    "    ''' Help convert text into suitable table caption '''\n",
    "    return \"<br><b>Table %d - %s</b>\" % (tablenum, msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup some inflation multipliers:\n",
    "\n",
    "**TODO** Fill in more values for updated multipliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multiply a dollar value to get the equivalent 2014 dollars\n",
    "# Original numbers from model; used to confirm model\n",
    "inflation_multipliers = {\n",
    "    2010: 1.092609, \n",
    "    2011: 1.059176,\n",
    "    2012: 1.037701,\n",
    "    2013: 1.022721,\n",
    "    2014: 1.0\n",
    "}\n",
    "\n",
    "# Updated inflation numbers should scale to 2015 dollars\n",
    "updated_inflation_multipliers = {\n",
    "    2001: 1.0,\n",
    "    2002: 1.0,\n",
    "    2003: 1.0,\n",
    "    2004: 1.0,\n",
    "    2005: 1.0,\n",
    "    2006: 1.0,\n",
    "    2007: 1.0,\n",
    "    2008: 1.0,\n",
    "    2009: 1.0, # 1.0's just to have a value for this key ...\n",
    "    2010: 1.092609, \n",
    "    2011: 1.059176,\n",
    "    2012: 1.037701,\n",
    "    2013: 1.022721,\n",
    "    2014: 1.0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global identifiers used throughout the project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Constants used to refer to US regions\n",
    "REGION_EAST = 'east'\n",
    "REGION_MIDWEST = 'midwest'\n",
    "REGION_SOUTH = 'south'\n",
    "REGION_WEST = 'west'\n",
    "REGION_BASE = 'base'   # USed for when a state is not in a region (Alaska, Hawaii mostly)\n",
    "\n",
    "# Create a state initial to region mapping to use for regional weighting\n",
    "state_to_region_mapping = defaultdict(constant_factory(REGION_BASE))\n",
    "    \n",
    "state_to_region_mapping.update(\n",
    "    { \n",
    "    'PA': REGION_EAST, 'NJ': REGION_EAST, 'NY': REGION_EAST, 'CT': REGION_EAST, 'MA': REGION_EAST,\n",
    "    'NH': REGION_EAST, 'VT': REGION_EAST, 'ME': REGION_EAST, 'RI': REGION_EAST, \n",
    "    'OH': REGION_MIDWEST, 'IL': REGION_MIDWEST, 'IN': REGION_MIDWEST, 'WI': REGION_MIDWEST, 'MI': REGION_MIDWEST,\n",
    "    'MN': REGION_MIDWEST, 'IA': REGION_MIDWEST, 'MO': REGION_MIDWEST, 'KS': REGION_MIDWEST, 'NE': REGION_MIDWEST,\n",
    "    'SD': REGION_MIDWEST, 'ND': REGION_MIDWEST,\n",
    "    'TX': REGION_SOUTH, 'OK': REGION_SOUTH, 'AR': REGION_SOUTH, 'LA': REGION_SOUTH, 'MS': REGION_SOUTH,\n",
    "    'AL': REGION_SOUTH, 'GA': REGION_SOUTH, 'FL': REGION_SOUTH, 'SC': REGION_SOUTH, 'NC': REGION_SOUTH,\n",
    "    'VA': REGION_SOUTH, 'WV': REGION_SOUTH, 'KY': REGION_SOUTH, 'TN': REGION_SOUTH, 'MD': REGION_SOUTH,\n",
    "    'DE': REGION_SOUTH,\n",
    "    'CA': REGION_WEST, 'OR': REGION_WEST, 'WA': REGION_WEST, 'NV': REGION_WEST, 'ID': REGION_WEST,\n",
    "    'UT': REGION_WEST, 'AZ': REGION_WEST, 'MT': REGION_WEST, 'WY': REGION_WEST, 'CO': REGION_WEST,\n",
    "    'NM': REGION_WEST, 'AK': REGION_BASE, 'HI': REGION_BASE\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets setup regional differences for the food data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multiply price of food by regional multipler to get better estimate of food costs\n",
    "food_regional_multipliers = {\n",
    "    REGION_EAST: 0.08,\n",
    "    REGION_WEST: 0.11,\n",
    "    REGION_SOUTH: -0.07,\n",
    "    REGION_MIDWEST: -0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "------\n",
    "\n",
    "------\n",
    "\n",
    "#  Data Collection\n",
    "\n",
    "The following sections will outline how I gathered the data for the various model parameters as well as other data we need to calculate their values. The original model was made for 2014 data and extending this data to the past means we need to be careful that any changes in the underlying data methodology of these parameters needs to be noted.\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "### Consumer Expenditure Report\n",
    "\n",
    "Wget commands used to get the Consumer Expenditure Reports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get CEX for 2013 and 2014 (XLSX format)\n",
    "for i in `seq 2013 2014`; do wget http://www.bls.gov/cex/$i/aggregate/cusize.xlsx -O ${i}_cex.xlsx; done\n",
    "\n",
    "# Get CEX for 2004 - 2012 (XLS format)\n",
    "for i in `seq 2004 2012`; do wget http://www.bls.gov/cex/$i/aggregate/cusize.xls -O ${i}_cex.xls; done\n",
    "\n",
    "# Get CEX for 2001 to 2003 (TXT format)\n",
    "for i in `seq 2001 2003`; do wget http://www.bls.gov/cex/aggregate/$i/cusize.txt -O ${i}_cex.txt; done\n",
    "\n",
    "\n",
    "# Get CEX region for 2013 and 2014 (XLSX format)\n",
    "for i in `seq 2013 2014`; do wget http://www.bls.gov/cex/$i/aggregate/region.xlsx -O ${i}_region_cex.xlsx; done\n",
    "\n",
    "# Get CEX region for 2004 - 2012 (XLS format)\n",
    "for i in `seq 2004 2012`; do wget http://www.bls.gov/cex/$i/aggregate/region.xls -O ${i}_region_cex.xls; done\n",
    "\n",
    "# Get CEX region for 2001 to 2003 (TXT format)\n",
    "for i in `seq 2001 2003`; do wget http://www.bls.gov/cex/aggregate/$i/region.txt -O ${i}_region_cex.txt; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USDA Food Plans\n",
    "\n",
    "Wget commands used to gather data files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Change command to get '10 - '15\n",
    "for i in {1..9}; do  wget http://www.cnpp.usda.gov/sites/default/files/usda_food_plans_cost_of_food/CostofFoodJun0$i.pdf; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Market Rent Data From HUD\n",
    "\n",
    "Below are the wget commands for getting the FMR data\n",
    "\n",
    "#### TODO \n",
    "\n",
    "* Extract counties -> state -> region mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in `seq 2014 2015`; do wget http://www.huduser.gov/portal/datasets/fmr/fmr${i}f/FY${i}_4050_RevFinal.xls -O fmr${i}.xlsx; done\n",
    "for i in `seq 2010 2013`; do wget http://www.huduser.gov/portal/datasets/fmr/fmr${i}f/FY${i}_4050_Final.xls -O fmr${i}.xlsx; done\n",
    "for i in `seq 2009 2009`; do wget http://www.huduser.gov/portal/datasets/fmr/fmr${i}r/FY${i}_4050_Rev_Final.xls -O fmr${i}.xlsx; done\n",
    "\n",
    "# GRRRR\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/fmr2008r/FMR_county_fy2008r_rdds.xls\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/fmr2007f/FY2007F_County_Town.xls\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/fmr2006r/FY2006_County_Town.xls\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/fmr2005r/Revised_FY2005_CntLevel.xls\n",
    "wget http://www.huduser.gov/portal/datasets/FMR/FMR2004F/FMR2004F_County.xls\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/FMR2003F_County.xls\n",
    "wget http://www.huduser.gov/portal/datasets/fmr/FMR2002F.xls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Counties dict will map county ID to useful infomation, mostly region\n",
    "counties = { }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medical Expenditure Panel Survey from the Agency for Healthcare Research and Quality\n",
    "\n",
    "Below are the wget commands used to download this data. This data will have to be further parsed from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to figure out what to do prior to 2006\n",
    "for i in `seq 2006 2014`; do \n",
    "    wget -O ${i}_txc1.html http://meps.ahrq.gov/mepsweb/data_stats/summ_tables/insr/state/series_10/${i}/txc1.htm; \n",
    "done \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Variable: Housing Cost\n",
    "\n",
    "Definition from the model:\n",
    "\n",
    "> We assumed  that  a  one  adult  family  would  rent a  single occupancy unit (zero bedrooms) for an individual adult household, that a two adult family would  rent a one bedroom apartment,\n",
    "\n",
    "FIPS code is just state code + county code + subcounty code (only post 2005)\n",
    "\n",
    "\n",
    "### DONE\n",
    "\n",
    "* download all data (DONE -in above section on data downloading)\n",
    "* figure out how to extract what we need from each XLS and extract CSV files (DONE)\n",
    "* import CSV files into pandas data frames (DONE for 2003-2014)\n",
    "* create global map of state to region (DONE)\n",
    "* 2002 fips code by matching counties via levishtein distance (DONE)\n",
    "\n",
    "### TODO\n",
    "\n",
    "* Look into 2005 and 2006 transition\n",
    "* figure out multi level index for data\n",
    "* subset all data to include counties that are across all years\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fair Market Rent data\n",
    "fmr_data = { }\n",
    "\n",
    "def pad_county(county):\n",
    "    ''' Pad counties to three digits when we need to construct one manually. '''\n",
    "    return '%03d' % county\n",
    "\n",
    "def pad_fips(fip):\n",
    "    ''' Add 99999 to end of fip code (which nullifies the subcounty identifier) '''\n",
    "    return int(str(fip) + '99999')\n",
    "\n",
    "# For now, only loading 2008 - 2014\n",
    "for year in range(2002, 2015):\n",
    "    with open(PROJECT_PATH + \"/data/fmr/fmr%d.csv\" % year, 'rb') as csvfile:\n",
    "        # Store dataframe from csv into dict\n",
    "        fmr_data[year] = pd.read_csv(csvfile)\n",
    "        \n",
    "        # Lower case headings to make life easier\n",
    "        fmr_data[year].columns = map(str.lower, fmr_data[year].columns)\n",
    "        \n",
    "        # Custom processing per year\n",
    "        if year > 2012:\n",
    "            fmr_data[year] = fmr_data[year][[\"fmr0\", \"county\", \"cousub\", \"countyname\", \"fips2000\", \"fips2010\", \"pop2010\", \"state\", \"state_alpha\"]]\n",
    "            \n",
    "            # TODO: should we do this?\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['fips2000']\n",
    "            \n",
    "            fmr_data[year] = fmr_data[year].query('cousub == 99999').reset_index(drop=True)\n",
    "        elif year > 2005:\n",
    "            fmr_data[year] = fmr_data[year][[\"fmr0\", \"county\", \"cousub\", \"countyname\", \"fips\", \"pop2000\", \"state\", \"state_alpha\"]]\n",
    "            fmr_data[year] = fmr_data[year].query('cousub == 99999').reset_index(drop=True)\n",
    "        elif year == 2005:\n",
    "            fmr_data[year] = fmr_data[year][[\"fmr_0bed\", \"county\", \"countyname\", \"pop2000\", \"state\", \"state_alpha\", \"stco\"\n",
    "]]\n",
    "            fmr_data[year].rename(columns={'stco':'fips', 'fmr_0bed': 'fmr'}, inplace=True)\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['fips'].map(pad_fips)\n",
    "        elif year == 2004:\n",
    "            fmr_data[year] = fmr_data[year][[\"new_fmr0\", \"county\", \"countyname\", \"pop100\", \"state\", \"state_alpha\"]]\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['state'].map(str) + fmr_data[year]['county'].map(pad_county)\n",
    "            fmr_data[year].rename(columns={'stco':'fips', 'new_fmr0': 'fmr'}, inplace=True)\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['fips'].map(pad_fips)\n",
    "        elif year == 2003:\n",
    "            fmr_data[year] = fmr_data[year][[\"fmr0\", \"county\", \"countyname\", \"pop\", \"state\", \"state_alpha\"]]\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['state'].map(str) + fmr_data[year]['county'].map(pad_county)\n",
    "            fmr_data[year]['fips'] = fmr_data[year]['fips'].map(pad_fips)\n",
    "        elif year == 2002:\n",
    "            # NOTE: we have to calculate FIPS codes by hand in cell below\n",
    "            fmr_data[year] = fmr_data[year][[\"fmr0br\", \"areaname\", \"st\"]]\n",
    "            fmr_data[year].rename(columns={'st':'state_alpha'}, inplace=True)\n",
    "\n",
    "        # Add region column\n",
    "        # METHOD: the defaultdict will use region_base if the state is not in the initial state to region mapping\n",
    "        fmr_data[year]['region'] = fmr_data[year]['state_alpha'].map(lambda x: state_to_region_mapping[x])\n",
    "\n",
    "# Print example\n",
    "# fmr_data[2003]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue with no fips for 2002\n",
    "\n",
    "We need to do some string matching to find FIPS codes for 2002, since they are not in the file. Exact matches work for 84% of the data. The other data is filled in via finding name with smallest levishtein distance. Used [py-editdist]( http://www.mindrot.org/projects/py-editdist) instead of nltk's implementation due to speed issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import editdist\n",
    "\n",
    "# Custom comparator to compare column of strings to given string\n",
    "def compare_lambda(y):\n",
    "    def compare(x):\n",
    "        return (x[0], x[1], editdist.distance(x[1], y))\n",
    "    return compare\n",
    "\n",
    "# Init list of fips we need to find and a bitmap of which 2002 counties we processes\n",
    "fips = [ None ] * len(fmr_data[2002]['areaname'])\n",
    "found_bitmap = [ False ] * len(fmr_data[2002]['areaname'])\n",
    "\n",
    "# For each count in 2002 ...\n",
    "for idx, countyname in enumerate(fmr_data[2002]['areaname']):\n",
    "    # See if any row mathes this countyname exactly\n",
    "    county_matches = fmr_data[2003]['countyname'].map(lambda x: x.lower()) == countyname.lower()\n",
    "    found = np.any(county_matches)\n",
    "    if found: \n",
    "        found_bitmap[idx] = True\n",
    "        fips[idx] = fmr_data[2003]['fips'][idx]\n",
    "\n",
    "\n",
    "# 84% found a match. can we do better with lev dist?\n",
    "# print np.sum(found_bitmap) / float(len(found_bitmap))\n",
    "\n",
    "# Get list of counties (as tuples) in 2003 which we try to match to\n",
    "good_counties = list(enumerate(fmr_data[2003]['countyname']))\n",
    "\n",
    "# For each county in 2002 ...\n",
    "for idx, countyname in enumerate(fmr_data[2002]['areaname']):\n",
    "    # If already matched, we skip; otherwise ...\n",
    "    if not found_bitmap[idx]:\n",
    "        # Get list of distances from 2002 countyname to all 2003 countynames\n",
    "        # NOTE: use of compare_lambda to create custom comparator that also \n",
    "        returns data in (idx, countyname, levdist) form\n",
    "        distances = map(compare_lambda(countyname.lower()), \n",
    "                        map(lambda x: (x[0], x[1].lower()), \n",
    "                            good_counties))\n",
    "        \n",
    "        # Find the minimum distance (with custom key to only compare third element, which is levdist)\n",
    "        min_distance = min(distances, key=lambda x: x[2])\n",
    "        \n",
    "        # Update bitmap and store appropriate FIPS code from 2003 \n",
    "        found_bitmap[idx] = True\n",
    "        fips[idx] = fmr_data[2003]['fips'][idx]\n",
    "\n",
    "# Add calculated fips to new column in 2002\n",
    "fmr_data[2002]['fips'] = fips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue with county change from 2005 to 2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff between 2003 and 2004 is: 2\n",
      "Diff between 2003 and 2004 is: 0\n",
      "\n",
      "Diff between 2004 and 2005 is: 12\n",
      "Diff between 2004 and 2005 is: 33\n",
      "\n",
      "Diff between 2005 and 2006 is: 39\n",
      "Diff between 2005 and 2006 is: 67\n",
      "\n",
      "Diff between 2006 and 2007 is: 0\n",
      "Diff between 2006 and 2007 is: 0\n",
      "\n",
      "Diff between 2007 and 2008 is: 0\n",
      "Diff between 2007 and 2008 is: 0\n",
      "\n",
      "Diff between 2008 and 2009 is: 0\n",
      "Diff between 2008 and 2009 is: 0\n",
      "\n",
      "Diff between 2009 and 2010 is: 0\n",
      "Diff between 2009 and 2010 is: 0\n",
      "\n",
      "Diff between 2010 and 2011 is: 5\n",
      "Diff between 2010 and 2011 is: 3\n",
      "\n",
      "Diff between 2011 and 2012 is: 0\n",
      "Diff between 2011 and 2012 is: 0\n",
      "\n",
      "Diff between 2012 and 2013 is: 0\n",
      "Diff between 2012 and 2013 is: 0\n",
      "\n",
      "Diff between 2013 and 2014 is: 0\n",
      "Diff between 2013 and 2014 is: 0\n",
      "\n",
      "set([7204999999, 7209599999, 7208399999, 7214199999, 7207199999, 7200199999, 5119599999, 7213199999, 7211799999, 7209399999, 5166099999, 7208199999, 7211599999, 7207999999, 7200999999, 5175099999, 5153099999, 5100599999, 7205799999, 7203999999, 7204399999, 7201999999, 5101599999, 5167899999, 7212399999, 5179099999, 5159599999, 5156099999, 7213399999, 7205599999, 7210999999, 5117599999, 7802099999, 5103599999, 7201599999, 7207399999, 5108999999, 5106999999, 7210799999])\n"
     ]
    }
   ],
   "source": [
    "for year in range(2003, 2014):\n",
    "    x = set(fmr_data[year]['fips'])\n",
    "    y = set(fmr_data[year+1]['fips'])\n",
    "    print(\"Diff between %d and %d is: %s\" % (year, year+1, len(y.difference(x))))\n",
    "    print(\"Diff between %d and %d is: %s\" % (year, year+1, len(x.difference(y))))\n",
    "    print\n",
    "\n",
    "# print(list(set(fmr_data[2005]['fips']))[0:10])\n",
    "# print(list(set(fmr_data[2006]['fips']))[0:10])\n",
    "\n",
    "print set(fmr_data[2006]['fips']).difference(set(fmr_data[2005]['fips']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Variable: Food\n",
    "\n",
    "Data for the food calculations have been successfully downloaded in PDF form. The main way to calculate this is, from the PDF:\n",
    "\n",
    ">Adult  food  consumption  costs  are  estimated  by  averaging  the  low - cost  plan  food  costs for  males  and  females  between  19  and  50\n",
    "\n",
    "Note, we add 20% to the values from the data sheets, since the notes on all published PDFs from the USDA state to add 20% to the listed values for individuals since:\n",
    "\n",
    ">The costs given are for individuals in 4-person families. For individuals in other size families, the following adjustments are suggested: 1-person—add 20 percent; ...\n",
    "\n",
    "The notes for the model also state that regional weights are applied to give a better estimate for food costs across the nation. The result of this section are values fo 2014 that match exactly tot he data given on the model website, so I am confident the implementation of the methodology below is correct.\n",
    "\n",
    "### Notes: Change of Methodology\n",
    "\n",
    "In 2006, the data from the USDA changed the age ranges for their healthy meal cost calculations. The differences in range are minimal and should not effect overall estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The base food cost (not regionally weighed) for nation (data pulled manually from PDFs)\n",
    "national_monthly_food_cost_per_year = {\n",
    "    2014: {\"base\": np.average([241.50, 209.80])},\n",
    "    2013: {\"base\": np.average([234.60, 203.70])},\n",
    "    2012: {\"base\": np.average([234.00, 203.00])},\n",
    "    2011: {\"base\": np.average([226.80, 196.90])},\n",
    "    2010: {\"base\": np.average([216.30, 187.70])},\n",
    "    2009: {\"base\": np.average([216.50, 187.90])},\n",
    "    2008: {\"base\": np.average([216.90, 189.60])},\n",
    "    2007: {\"base\": np.average([200.20, 174.10])},\n",
    "    2006: {\"base\": np.average([189.70, 164.80])},\n",
    "    2005: {\"base\": np.average([186.20, 162.10])},\n",
    "    2004: {\"base\": np.average([183.10, 159.50])},\n",
    "    2003: {\"base\": np.average([174.20, 151.70])},\n",
    "    2002: {\"base\": np.average([170.30, 148.60])},\n",
    "    2001: {\"base\": np.average([166.80, 145.60])},\n",
    "}\n",
    "\n",
    "# Create ordered dict to make sure we process things in order\n",
    "national_monthly_food_cost_per_year = OrderedDict(sorted(national_monthly_food_cost_per_year.items(), \n",
    "                                                        key=lambda t: t[0]))\n",
    "\n",
    "# Adjust the data according to notes above\n",
    "for year in national_monthly_food_cost_per_year:\n",
    "    # Inflation and 20% adjustment\n",
    "    national_monthly_food_cost_per_year[year][\"base\"] = \\\n",
    "        national_monthly_food_cost_per_year[year][\"base\"] * 1.20 * updated_inflation_multipliers[year]\n",
    "\n",
    "    # Regional adjustment\n",
    "    national_monthly_food_cost_per_year[year][\"regional\"] = { }\n",
    "    for region in food_regional_multipliers:\n",
    "        national_monthly_food_cost_per_year[year][\"regional\"][region] = \\\n",
    "            national_monthly_food_cost_per_year[year][\"base\"] * (1 + food_regional_multipliers[region])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In yearly form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Year</th>\n",
       "        <th>Food Cost (per year)</th>\n",
       "        <th>Food Cost (west)</th>\n",
       "        <th>Food Cost (east)</th>\n",
       "        <th>Food Cost (midwest)</th>\n",
       "        <th>Food Cost (south)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2001</td>\n",
       "        <td>2249.0</td>\n",
       "        <td>2497.0</td>\n",
       "        <td>2429.0</td>\n",
       "        <td>2137.0</td>\n",
       "        <td>2092.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2002</td>\n",
       "        <td>2296.0</td>\n",
       "        <td>2549.0</td>\n",
       "        <td>2480.0</td>\n",
       "        <td>2181.0</td>\n",
       "        <td>2135.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2003</td>\n",
       "        <td>2346.0</td>\n",
       "        <td>2605.0</td>\n",
       "        <td>2534.0</td>\n",
       "        <td>2229.0</td>\n",
       "        <td>2182.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2004</td>\n",
       "        <td>2467.0</td>\n",
       "        <td>2738.0</td>\n",
       "        <td>2664.0</td>\n",
       "        <td>2343.0</td>\n",
       "        <td>2294.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2005</td>\n",
       "        <td>2508.0</td>\n",
       "        <td>2784.0</td>\n",
       "        <td>2708.0</td>\n",
       "        <td>2382.0</td>\n",
       "        <td>2332.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2006</td>\n",
       "        <td>2552.0</td>\n",
       "        <td>2833.0</td>\n",
       "        <td>2757.0</td>\n",
       "        <td>2425.0</td>\n",
       "        <td>2374.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2007</td>\n",
       "        <td>2695.0</td>\n",
       "        <td>2991.0</td>\n",
       "        <td>2911.0</td>\n",
       "        <td>2560.0</td>\n",
       "        <td>2506.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2008</td>\n",
       "        <td>2927.0</td>\n",
       "        <td>3249.0</td>\n",
       "        <td>3161.0</td>\n",
       "        <td>2780.0</td>\n",
       "        <td>2722.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2009</td>\n",
       "        <td>2912.0</td>\n",
       "        <td>3232.0</td>\n",
       "        <td>3145.0</td>\n",
       "        <td>2766.0</td>\n",
       "        <td>2708.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2010</td>\n",
       "        <td>3178.0</td>\n",
       "        <td>3528.0</td>\n",
       "        <td>3432.0</td>\n",
       "        <td>3019.0</td>\n",
       "        <td>2956.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2011</td>\n",
       "        <td>3231.0</td>\n",
       "        <td>3587.0</td>\n",
       "        <td>3490.0</td>\n",
       "        <td>3070.0</td>\n",
       "        <td>3005.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2012</td>\n",
       "        <td>3265.0</td>\n",
       "        <td>3624.0</td>\n",
       "        <td>3526.0</td>\n",
       "        <td>3102.0</td>\n",
       "        <td>3036.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2013</td>\n",
       "        <td>3227.0</td>\n",
       "        <td>3582.0</td>\n",
       "        <td>3486.0</td>\n",
       "        <td>3066.0</td>\n",
       "        <td>3002.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2014</td>\n",
       "        <td>3249.0</td>\n",
       "        <td>3607.0</td>\n",
       "        <td>3509.0</td>\n",
       "        <td>3087.0</td>\n",
       "        <td>3022.0</td>\n",
       "    </tr>\n",
       "</table><br><b>Table 1 - Food Data Loaded from USDA Pricing on Meals</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print it nicely in yearly costs\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Year\", national_monthly_food_cost_per_year.keys())\n",
    "pt.add_column(\"Food Cost (per year)\", [np.round(x[\"base\"] * 12) for x in national_monthly_food_cost_per_year.values()])\n",
    "for region in food_regional_multipliers:\n",
    "    pt.add_column(\"Food Cost (%s)\" % region, [np.round(x[\"regional\"][region] * 12) for x in national_monthly_food_cost_per_year.values()])\n",
    "\n",
    "# Print as HTML\n",
    "HTML(pt.get_html_string() + caption(\"Food Data Loaded from USDA Pricing on Meals\", 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Variable: Transportation Cost\n",
    "\n",
    "Looking at the (1) Cars and trucks (used), (2) gasoline and motor oil, (3) other vehicle expenses, and (4)  public  transportation fields under \"Transportation\" in the 2014 Consumer Expenditure Report, we can pull out information from each to model the claculation done in the original model. For each sub-variable, we get the amount of money (in millions) and the percentgae of that that single adults spend. After multiple those numbers (accounting for units) and dividiing by the total number of single adults in the survey gives us a mean total cost per adult.\n",
    "\n",
    "The original model takes into account regional drift by scaling based on each regions. NOTE: See todo in this section\n",
    "\n",
    "Since this data reflects conditions in 2013, we account for inflation to get the 2014 estimate that is produced in the original model.\n",
    "\n",
    "### TODO:\n",
    "\n",
    "* Figure out how to do regional differences correctly. Emailed model creator for clarification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5209.92768399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Year</th>\n",
       "        <th>Trans Cost (base)</th>\n",
       "        <th>Trans Cost (east)</th>\n",
       "        <th>Trans Cost (midwest)</th>\n",
       "        <th>Trans Cost (south)</th>\n",
       "        <th>Trans Cost (west)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2012</td>\n",
       "        <td>4326.89007326</td>\n",
       "        <td>4031.87484099</td>\n",
       "        <td>4425.22848402</td>\n",
       "        <td>4646.48990822</td>\n",
       "        <td>4056.45944368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2013</td>\n",
       "        <td>4037.18458744</td>\n",
       "        <td>3728.45870723</td>\n",
       "        <td>4013.43644281</td>\n",
       "        <td>4345.91046766</td>\n",
       "        <td>3823.45128575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2014_ideal</td>\n",
       "        <td>0.0</td>\n",
       "        <td>3764</td>\n",
       "        <td>4569</td>\n",
       "        <td>4697</td>\n",
       "        <td>4054</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transportation data from 2014 survey is for year 2013, etc\n",
    "cex = {\n",
    "    2012: {\n",
    "        \"single_adults\": 37770.0,\n",
    "        \"transport\": {\n",
    "            \"used_car\": 209764.0,\n",
    "            \"gasoline\": 328170.0,\n",
    "            \"other_vehicle\": 324668.0,\n",
    "            \"public\": 67486.0,\n",
    "            \"used_car_percent\": 0.152,\n",
    "            \"gasoline_percent\": 0.158,\n",
    "            \"other_vehicle_percent\": 0.191,\n",
    "            \"public_percent\": 0.174,\n",
    "            \"regional\": {\n",
    "                REGION_EAST:   16.4 / 17.6,  \n",
    "                REGION_MIDWEST: 18.0 / 17.6,\n",
    "                REGION_SOUTH: 18.9 / 17.6,\n",
    "                REGION_WEST: 16.5 / 17.6,\n",
    "            }\n",
    "        }\n",
    "\n",
    "    },\n",
    "    2013: {\n",
    "        \"single_adults\": 37884.0,\n",
    "        \"transport\": {\n",
    "            \"used_car\": 214524.0,\n",
    "            \"gasoline\": 313481.0,\n",
    "            \"other_vehicle\": 345454.0,\n",
    "            \"public\": 73842.0,\n",
    "            \"used_car_percent\": 0.146,\n",
    "            \"gasoline_percent\": 0.157,\n",
    "            \"other_vehicle_percent\": 0.163,\n",
    "            \"public_percent\": 0.172,\n",
    "            \"regional\": {\n",
    "                REGION_EAST: 15.7 / 17.0,     # 0.923\n",
    "                REGION_MIDWEST: 16.9 / 17.0,  # 0.994\n",
    "                REGION_SOUTH: 18.3 / 17.0,    # 1.076\n",
    "                REGION_WEST: 16.1 / 17.0,     # 0.947\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Ideal numbers from model\n",
    "ideal_transport_2013 = (3764, 4569, 4697, 4054)\n",
    "\n",
    "# Base price for transport\n",
    "transportation_costs = defaultdict(dict)\n",
    "\n",
    "for year in cex:\n",
    "    transportation_costs[year][\"base\"] = \\\n",
    "        (1000000 * ((cex[year][\"transport\"][\"used_car\"] * cex[year][\"transport\"][\"used_car_percent\"]) + \\\n",
    "                    (cex[year][\"transport\"][\"gasoline\"] * cex[year][\"transport\"][\"gasoline_percent\"]) + \\\n",
    "                    (cex[year][\"transport\"][\"other_vehicle\"] * cex[year][\"transport\"][\"other_vehicle_percent\"] ) + \\\n",
    "                    (cex[year][\"transport\"][\"public\"] * cex[year][\"transport\"][\"public_percent\"] )) /  float(cex[year][\"single_adults\"] * 1000) ) * inflation_multipliers[year]\n",
    "\n",
    "    # Account for regional drift\n",
    "    for region in cex[year][\"transport\"][\"regional\"]:\n",
    "        transportation_costs[year][region] = transportation_costs[year][\"base\"] * cex[year][\"transport\"][\"regional\"][region]\n",
    "\n",
    "transportation_costs[\"2014_ideal\"][\"base\"] = 0.0\n",
    "transportation_costs[\"2014_ideal\"][REGION_EAST] = ideal_transport_2013[0]\n",
    "transportation_costs[\"2014_ideal\"][REGION_MIDWEST] = ideal_transport_2013[1]\n",
    "transportation_costs[\"2014_ideal\"][REGION_SOUTH] = ideal_transport_2013[2]\n",
    "transportation_costs[\"2014_ideal\"][REGION_WEST] = ideal_transport_2013[3]\n",
    "\n",
    "# Print it nicely\n",
    "errors = []\n",
    "pt = PrettyTable()\n",
    "pt.add_column(\"Year\", transportation_costs.keys())\n",
    "for region in sorted(transportation_costs[2013].keys()):\n",
    "    data = [ transportation_costs[year][region] for year in transportation_costs  ]\n",
    "    pt.add_column(\"Trans Cost (%s)\" % region, data)\n",
    "    errors.append(transportation_costs[\"2014_ideal\"][region] - data[-2])\n",
    "\n",
    "print(sum([np.abs(error) for error in errors]))\n",
    "\n",
    "# Print as HTML\n",
    "HTML(pt.get_html_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing theory about regional difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7516.27558175\n"
     ]
    }
   ],
   "source": [
    "# Order: NE, MW, S, W\n",
    "used_car_rations = (2.5 / 3.2, 3.5 / 3.2, 3.5 / 3.2, 2.9 / 3.2)\n",
    "gas_rations = (3.8 / 4.6, 4.7 / 4.6, 5.2 / 4.6, 4.5 / 4.6)\n",
    "other_rations = (5.2 / 5.1, 5.0  / 5.1, 5.1 / 5.1,  5.1 / 5.1)\n",
    "public_rations = (1.6/1.1,  0.9/1.1,  0.8/1.1, 1.2/1.1)\n",
    "\n",
    "error = []\n",
    "for region in range(4):\n",
    "    val = (1000000 * \n",
    "         ( (\n",
    "            (cex[2013][\"transport\"][\"used_car\"] * cex[2013][\"transport\"][\"used_car_percent\"] * used_car_rations[region]) + \\\n",
    "            (cex[2013][\"transport\"][\"gasoline\"] * cex[2013][\"transport\"][\"gasoline_percent\"] * gas_rations[region]) + \\\n",
    "            (cex[2013][\"transport\"][\"other_vehicle\"] * cex[2013][\"transport\"][\"other_vehicle_percent\"] * other_rations[region]) + \\\n",
    "            (cex[2013][\"transport\"][\"public\"] * cex[2013][\"transport\"][\"public_percent\"] * public_rations[region])\n",
    "        ) /  (float(cex[2013][\"single_adults\"] * 1000)) ) * inflation_multipliers[2013])\n",
    "    errors.append( val - ideal_transport_2013[region] )\n",
    "\n",
    "print(sum([np.abs(error) for error in errors]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15698618246\n",
      "0.169646358465\n",
      "0.183495040788\n",
      "0.160418732272\n",
      "0.169646358465\n"
     ]
    }
   ],
   "source": [
    "# calculate regional diff values from aggregated data (since 'combined' only goes back to 2012)\n",
    "print 1/ (6790803*1000000*20.1 / (1152035*1000000*18.6))\n",
    "print 1/ (6790803*1000000*21.7 / (1152035*1000000*21.7))\n",
    "print 1/ (6790803*1000000*34.3 / (1152035*1000000*37.1))\n",
    "print 1/ (6790803*1000000*23.9 / (1152035*1000000*22.6))\n",
    "\n",
    "\n",
    "print 1152035/6790803.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Variable: Child Care Cost\n",
    "\n",
    "Manually download PDFs from ChildCareAware.org. Sadly, they only go back to 2010. I can now either:\n",
    "\n",
    "* have to find other estimates of child care costs from pre-2010 (prefered)\n",
    "* check if the Consumer Expenditure Survey has data on this\n",
    "* impute the data (dont think this is a good idea)\n",
    "* limit the analysis going back to 2010 (which seems limiting since other data, like the Consumer Expenditure Survey in 2014 provides 2013 data and that is the latest currently).\n",
    "\n",
    "Currently I am only focusing on modeling costs for a single adult (an assumption I made early on) since I am interested in trends, and the other 'family configurations' are just linear combinations of the costs for one adult and for one child. However if I wanted to extend the numbers for 1 adult + 1 child, I would have to look into this further. For now I'll move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Variable: Insurance Premiums\n",
    "\n",
    "The model uses data from the Medical Expenditure Panel Survey from the Agency for Healthcare Research and Quality (searchable [here](http://meps.ahrq.gov/mepsweb/data_stats/quick_tables_search.jsp?component=2&subcomponent=2)). Specifically, the model assumes a single adult's insurance costs are best estimated from Table X.C.1 Employee contribution distributions (in dollars) for private-sector employees enrolled in single coverage. This survey gives the mean cost for a single adult per state.\n",
    "\n",
    "Below is code on processing each html file.\n",
    "\n",
    "**PROBLEMS**\n",
    "\n",
    "* One problem is that in 2007 this survey was not done.\n",
    "    * Linearly impute data from 2006 and 2008? Seems resonable if we can assume that costs tend to go up every year and not go down, which makes it seem likely that 2007 values would be bounded by the previous and next year\n",
    "* Another problem is that this portion of the survey started in 2006\n",
    "    * Find another data source for pre 2006?\n",
    "    * Limit analysis to 2006 - 2014, which is nearly a decade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process HTML files with BeautifulSoup\n",
    "insurance_costs = {}\n",
    "insurance_costs_path = os.path.join(PROJECT_PATH, \"data/insurance\")\n",
    "\n",
    "# Loop thru all the files\n",
    "for filename in os.listdir(insurance_costs_path):\n",
    "    states = {}\n",
    "    \n",
    "    # File is for what year?\n",
    "    year = int(filename.split('_')[0])\n",
    "    \n",
    "    # Open file\n",
    "    full_filename = os.path.join(insurance_costs_path, filename)\n",
    "    f = open(full_filename, \"r\")\n",
    "    \n",
    "    # Import into BeautifulSoup\n",
    "    data = f.readlines()\n",
    "    soup = BeautifulSoup(''.join(data))\n",
    "\n",
    "    # Works for years 2010 - 2014\n",
    "    if year in range(2010, 2015):\n",
    "        for tr in soup.find_all('tr'):\n",
    "            # State is located in the TR element\n",
    "            state = tr.get_text().split(\"\\n\")[1].lower().strip()\n",
    "            \n",
    "            # Find the data, but if you can't, skip it\n",
    "            td = tr.find_all('td')\n",
    "            value = None\n",
    "            if td: \n",
    "                try:\n",
    "                    value = float(td[0].get_text().strip().replace(\",\", \"\"))\n",
    "                    \n",
    "                    # Account for inflation and round up\n",
    "                    value = np.round(value * updated_inflation_multipliers[year])\n",
    "                except ValueError as e:\n",
    "                    continue\n",
    "\n",
    "            # We need to stop processing after the first chunk or if we couldnt get a value\n",
    "            if state not in states and value:\n",
    "                states[state] = value\n",
    "    # Works for 2006, 2008 - 2009\n",
    "    else:\n",
    "        for tr in soup.find_all('tr'):\n",
    "            td = tr.find_all('td')\n",
    "\n",
    "            value = None\n",
    "            if len(td) > 2: \n",
    "                # Same as above, but state is fist TD, not in TR\n",
    "                state = td[0].get_text().lower().strip()\n",
    "                try:\n",
    "                    value = float(td[1].get_text().strip().replace(\",\", \"\"))\n",
    "                    \n",
    "                    # Account for inflation\n",
    "                    value = np.round(value * updated_inflation_multipliers[year])\n",
    "                except ValueError as e:\n",
    "                    continue\n",
    "\n",
    "            if state not in states and value:\n",
    "                states[state] = value\n",
    "\n",
    "    # Add data from file to global dict\n",
    "    insurance_costs[year] = states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>Year</th>\n",
       "        <th>alabama</th>\n",
       "        <th>alaska</th>\n",
       "        <th>arizona</th>\n",
       "        <th>arkansas</th>\n",
       "        <th>california</th>\n",
       "        <th>colorado</th>\n",
       "        <th>connecticut</th>\n",
       "        <th>delaware</th>\n",
       "        <th>florida</th>\n",
       "        <th>georgia</th>\n",
       "        <th>hawaii</th>\n",
       "        <th>idaho</th>\n",
       "        <th>illinois</th>\n",
       "        <th>indiana</th>\n",
       "        <th>iowa</th>\n",
       "        <th>kansas</th>\n",
       "        <th>kentucky</th>\n",
       "        <th>louisiana</th>\n",
       "        <th>maine</th>\n",
       "        <th>maryland</th>\n",
       "        <th>massachusetts</th>\n",
       "        <th>michigan</th>\n",
       "        <th>minnesota</th>\n",
       "        <th>mississippi</th>\n",
       "        <th>missouri</th>\n",
       "        <th>montana</th>\n",
       "        <th>nebraska</th>\n",
       "        <th>nevada</th>\n",
       "        <th>new hampshire</th>\n",
       "        <th>new jersey</th>\n",
       "        <th>new mexico</th>\n",
       "        <th>new york</th>\n",
       "        <th>north carolina</th>\n",
       "        <th>north dakota</th>\n",
       "        <th>ohio</th>\n",
       "        <th>oklahoma</th>\n",
       "        <th>oregon</th>\n",
       "        <th>pennsylvania</th>\n",
       "        <th>rhode island</th>\n",
       "        <th>south carolina</th>\n",
       "        <th>south dakota</th>\n",
       "        <th>tennessee</th>\n",
       "        <th>texas</th>\n",
       "        <th>utah</th>\n",
       "        <th>vermont</th>\n",
       "        <th>virginia</th>\n",
       "        <th>washington</th>\n",
       "        <th>west virginia</th>\n",
       "        <th>wisconsin</th>\n",
       "        <th>wyoming</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2006</td>\n",
       "        <td>891.0</td>\n",
       "        <td>714.0</td>\n",
       "        <td>803.0</td>\n",
       "        <td>699.0</td>\n",
       "        <td>658.0</td>\n",
       "        <td>717.0</td>\n",
       "        <td>862.0</td>\n",
       "        <td>735.0</td>\n",
       "        <td>860.0</td>\n",
       "        <td>862.0</td>\n",
       "        <td>366.0</td>\n",
       "        <td>565.0</td>\n",
       "        <td>822.0</td>\n",
       "        <td>833.0</td>\n",
       "        <td>784.0</td>\n",
       "        <td>765.0</td>\n",
       "        <td>691.0</td>\n",
       "        <td>755.0</td>\n",
       "        <td>1072.0</td>\n",
       "        <td>898.0</td>\n",
       "        <td>1011.0</td>\n",
       "        <td>682.0</td>\n",
       "        <td>810.0</td>\n",
       "        <td>727.0</td>\n",
       "        <td>703.0</td>\n",
       "        <td>598.0</td>\n",
       "        <td>873.0</td>\n",
       "        <td>551.0</td>\n",
       "        <td>1004.0</td>\n",
       "        <td>902.0</td>\n",
       "        <td>726.0</td>\n",
       "        <td>965.0</td>\n",
       "        <td>704.0</td>\n",
       "        <td>675.0</td>\n",
       "        <td>781.0</td>\n",
       "        <td>650.0</td>\n",
       "        <td>547.0</td>\n",
       "        <td>881.0</td>\n",
       "        <td>862.0</td>\n",
       "        <td>810.0</td>\n",
       "        <td>718.0</td>\n",
       "        <td>745.0</td>\n",
       "        <td>728.0</td>\n",
       "        <td>826.0</td>\n",
       "        <td>738.0</td>\n",
       "        <td>981.0</td>\n",
       "        <td>623.0</td>\n",
       "        <td>825.0</td>\n",
       "        <td>885.0</td>\n",
       "        <td>655.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2008</td>\n",
       "        <td>959.0</td>\n",
       "        <td>814.0</td>\n",
       "        <td>811.0</td>\n",
       "        <td>781.0</td>\n",
       "        <td>741.0</td>\n",
       "        <td>998.0</td>\n",
       "        <td>992.0</td>\n",
       "        <td>885.0</td>\n",
       "        <td>1065.0</td>\n",
       "        <td>972.0</td>\n",
       "        <td>451.0</td>\n",
       "        <td>476.0</td>\n",
       "        <td>954.0</td>\n",
       "        <td>950.0</td>\n",
       "        <td>756.0</td>\n",
       "        <td>807.0</td>\n",
       "        <td>806.0</td>\n",
       "        <td>868.0</td>\n",
       "        <td>1054.0</td>\n",
       "        <td>964.0</td>\n",
       "        <td>1110.0</td>\n",
       "        <td>735.0</td>\n",
       "        <td>891.0</td>\n",
       "        <td>749.0</td>\n",
       "        <td>956.0</td>\n",
       "        <td>583.0</td>\n",
       "        <td>1010.0</td>\n",
       "        <td>863.0</td>\n",
       "        <td>1264.0</td>\n",
       "        <td>1033.0</td>\n",
       "        <td>950.0</td>\n",
       "        <td>947.0</td>\n",
       "        <td>827.0</td>\n",
       "        <td>754.0</td>\n",
       "        <td>885.0</td>\n",
       "        <td>787.0</td>\n",
       "        <td>612.0</td>\n",
       "        <td>852.0</td>\n",
       "        <td>1050.0</td>\n",
       "        <td>849.0</td>\n",
       "        <td>887.0</td>\n",
       "        <td>914.0</td>\n",
       "        <td>844.0</td>\n",
       "        <td>752.0</td>\n",
       "        <td>986.0</td>\n",
       "        <td>988.0</td>\n",
       "        <td>569.0</td>\n",
       "        <td>1049.0</td>\n",
       "        <td>1069.0</td>\n",
       "        <td>717.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2009</td>\n",
       "        <td>1025.0</td>\n",
       "        <td>842.0</td>\n",
       "        <td>851.0</td>\n",
       "        <td>750.0</td>\n",
       "        <td>795.0</td>\n",
       "        <td>971.0</td>\n",
       "        <td>1082.0</td>\n",
       "        <td>1101.0</td>\n",
       "        <td>969.0</td>\n",
       "        <td>963.0</td>\n",
       "        <td>461.0</td>\n",
       "        <td>762.0</td>\n",
       "        <td>1008.0</td>\n",
       "        <td>1070.0</td>\n",
       "        <td>855.0</td>\n",
       "        <td>976.0</td>\n",
       "        <td>1000.0</td>\n",
       "        <td>956.0</td>\n",
       "        <td>981.0</td>\n",
       "        <td>1105.0</td>\n",
       "        <td>1321.0</td>\n",
       "        <td>946.0</td>\n",
       "        <td>994.0</td>\n",
       "        <td>994.0</td>\n",
       "        <td>999.0</td>\n",
       "        <td>768.0</td>\n",
       "        <td>873.0</td>\n",
       "        <td>842.0</td>\n",
       "        <td>1087.0</td>\n",
       "        <td>1045.0</td>\n",
       "        <td>934.0</td>\n",
       "        <td>1075.0</td>\n",
       "        <td>998.0</td>\n",
       "        <td>860.0</td>\n",
       "        <td>1065.0</td>\n",
       "        <td>815.0</td>\n",
       "        <td>627.0</td>\n",
       "        <td>917.0</td>\n",
       "        <td>1207.0</td>\n",
       "        <td>898.0</td>\n",
       "        <td>890.0</td>\n",
       "        <td>1010.0</td>\n",
       "        <td>991.0</td>\n",
       "        <td>772.0</td>\n",
       "        <td>1008.0</td>\n",
       "        <td>1060.0</td>\n",
       "        <td>640.0</td>\n",
       "        <td>1085.0</td>\n",
       "        <td>1011.0</td>\n",
       "        <td>729.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2010</td>\n",
       "        <td>1193.0</td>\n",
       "        <td>909.0</td>\n",
       "        <td>974.0</td>\n",
       "        <td>967.0</td>\n",
       "        <td>1145.0</td>\n",
       "        <td>965.0</td>\n",
       "        <td>1348.0</td>\n",
       "        <td>1289.0</td>\n",
       "        <td>1172.0</td>\n",
       "        <td>1054.0</td>\n",
       "        <td>476.0</td>\n",
       "        <td>909.0</td>\n",
       "        <td>1224.0</td>\n",
       "        <td>1231.0</td>\n",
       "        <td>1016.0</td>\n",
       "        <td>1011.0</td>\n",
       "        <td>968.0</td>\n",
       "        <td>1356.0</td>\n",
       "        <td>1319.0</td>\n",
       "        <td>1180.0</td>\n",
       "        <td>1311.0</td>\n",
       "        <td>1039.0</td>\n",
       "        <td>1118.0</td>\n",
       "        <td>1125.0</td>\n",
       "        <td>1054.0</td>\n",
       "        <td>1140.0</td>\n",
       "        <td>1184.0</td>\n",
       "        <td>838.0</td>\n",
       "        <td>1187.0</td>\n",
       "        <td>1200.0</td>\n",
       "        <td>1288.0</td>\n",
       "        <td>1187.0</td>\n",
       "        <td>1012.0</td>\n",
       "        <td>974.0</td>\n",
       "        <td>1040.0</td>\n",
       "        <td>1140.0</td>\n",
       "        <td>927.0</td>\n",
       "        <td>1042.0</td>\n",
       "        <td>1253.0</td>\n",
       "        <td>1099.0</td>\n",
       "        <td>1036.0</td>\n",
       "        <td>1060.0</td>\n",
       "        <td>1132.0</td>\n",
       "        <td>1187.0</td>\n",
       "        <td>1201.0</td>\n",
       "        <td>1217.0</td>\n",
       "        <td>815.0</td>\n",
       "        <td>1019.0</td>\n",
       "        <td>1283.0</td>\n",
       "        <td>876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2011</td>\n",
       "        <td>1195.0</td>\n",
       "        <td>1146.0</td>\n",
       "        <td>1209.0</td>\n",
       "        <td>1028.0</td>\n",
       "        <td>1032.0</td>\n",
       "        <td>1122.0</td>\n",
       "        <td>1273.0</td>\n",
       "        <td>1183.0</td>\n",
       "        <td>1202.0</td>\n",
       "        <td>1314.0</td>\n",
       "        <td>578.0</td>\n",
       "        <td>936.0</td>\n",
       "        <td>1278.0</td>\n",
       "        <td>1098.0</td>\n",
       "        <td>1142.0</td>\n",
       "        <td>1048.0</td>\n",
       "        <td>1174.0</td>\n",
       "        <td>1289.0</td>\n",
       "        <td>1179.0</td>\n",
       "        <td>1310.0</td>\n",
       "        <td>1523.0</td>\n",
       "        <td>1166.0</td>\n",
       "        <td>1151.0</td>\n",
       "        <td>1045.0</td>\n",
       "        <td>1223.0</td>\n",
       "        <td>872.0</td>\n",
       "        <td>1111.0</td>\n",
       "        <td>1093.0</td>\n",
       "        <td>1310.0</td>\n",
       "        <td>1281.0</td>\n",
       "        <td>1346.0</td>\n",
       "        <td>1218.0</td>\n",
       "        <td>1124.0</td>\n",
       "        <td>1045.0</td>\n",
       "        <td>1193.0</td>\n",
       "        <td>1096.0</td>\n",
       "        <td>925.0</td>\n",
       "        <td>1127.0</td>\n",
       "        <td>1470.0</td>\n",
       "        <td>1299.0</td>\n",
       "        <td>1191.0</td>\n",
       "        <td>1092.0</td>\n",
       "        <td>1058.0</td>\n",
       "        <td>1013.0</td>\n",
       "        <td>1293.0</td>\n",
       "        <td>1145.0</td>\n",
       "        <td>917.0</td>\n",
       "        <td>1049.0</td>\n",
       "        <td>1161.0</td>\n",
       "        <td>928.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2012</td>\n",
       "        <td>1279.0</td>\n",
       "        <td>1208.0</td>\n",
       "        <td>1200.0</td>\n",
       "        <td>1024.0</td>\n",
       "        <td>1035.0</td>\n",
       "        <td>1148.0</td>\n",
       "        <td>1368.0</td>\n",
       "        <td>1373.0</td>\n",
       "        <td>1213.0</td>\n",
       "        <td>1160.0</td>\n",
       "        <td>535.0</td>\n",
       "        <td>962.0</td>\n",
       "        <td>1190.0</td>\n",
       "        <td>1201.0</td>\n",
       "        <td>1234.0</td>\n",
       "        <td>1340.0</td>\n",
       "        <td>1149.0</td>\n",
       "        <td>1118.0</td>\n",
       "        <td>1128.0</td>\n",
       "        <td>1157.0</td>\n",
       "        <td>1566.0</td>\n",
       "        <td>1099.0</td>\n",
       "        <td>1258.0</td>\n",
       "        <td>1117.0</td>\n",
       "        <td>1175.0</td>\n",
       "        <td>826.0</td>\n",
       "        <td>1183.0</td>\n",
       "        <td>1063.0</td>\n",
       "        <td>1308.0</td>\n",
       "        <td>1269.0</td>\n",
       "        <td>1263.0</td>\n",
       "        <td>1301.0</td>\n",
       "        <td>1033.0</td>\n",
       "        <td>1010.0</td>\n",
       "        <td>1276.0</td>\n",
       "        <td>1137.0</td>\n",
       "        <td>871.0</td>\n",
       "        <td>1102.0</td>\n",
       "        <td>1385.0</td>\n",
       "        <td>1192.0</td>\n",
       "        <td>1260.0</td>\n",
       "        <td>1080.0</td>\n",
       "        <td>1051.0</td>\n",
       "        <td>1177.0</td>\n",
       "        <td>1289.0</td>\n",
       "        <td>1306.0</td>\n",
       "        <td>910.0</td>\n",
       "        <td>1151.0</td>\n",
       "        <td>1320.0</td>\n",
       "        <td>1111.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2013</td>\n",
       "        <td>1410.0</td>\n",
       "        <td>1102.0</td>\n",
       "        <td>1102.0</td>\n",
       "        <td>978.0</td>\n",
       "        <td>1116.0</td>\n",
       "        <td>1188.0</td>\n",
       "        <td>1536.0</td>\n",
       "        <td>1459.0</td>\n",
       "        <td>1440.0</td>\n",
       "        <td>1247.0</td>\n",
       "        <td>441.0</td>\n",
       "        <td>997.0</td>\n",
       "        <td>1331.0</td>\n",
       "        <td>1160.0</td>\n",
       "        <td>1224.0</td>\n",
       "        <td>1106.0</td>\n",
       "        <td>1243.0</td>\n",
       "        <td>1242.0</td>\n",
       "        <td>1144.0</td>\n",
       "        <td>1338.0</td>\n",
       "        <td>1683.0</td>\n",
       "        <td>1178.0</td>\n",
       "        <td>1260.0</td>\n",
       "        <td>1122.0</td>\n",
       "        <td>1060.0</td>\n",
       "        <td>902.0</td>\n",
       "        <td>1190.0</td>\n",
       "        <td>1332.0</td>\n",
       "        <td>1447.0</td>\n",
       "        <td>1282.0</td>\n",
       "        <td>1142.0</td>\n",
       "        <td>1320.0</td>\n",
       "        <td>1088.0</td>\n",
       "        <td>992.0</td>\n",
       "        <td>1077.0</td>\n",
       "        <td>1086.0</td>\n",
       "        <td>822.0</td>\n",
       "        <td>1098.0</td>\n",
       "        <td>1433.0</td>\n",
       "        <td>1163.0</td>\n",
       "        <td>1378.0</td>\n",
       "        <td>1194.0</td>\n",
       "        <td>1161.0</td>\n",
       "        <td>1114.0</td>\n",
       "        <td>1197.0</td>\n",
       "        <td>1272.0</td>\n",
       "        <td>695.0</td>\n",
       "        <td>1076.0</td>\n",
       "        <td>1248.0</td>\n",
       "        <td>1083.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>2014</td>\n",
       "        <td>1362.0</td>\n",
       "        <td>1286.0</td>\n",
       "        <td>1096.0</td>\n",
       "        <td>958.0</td>\n",
       "        <td>1129.0</td>\n",
       "        <td>1244.0</td>\n",
       "        <td>1305.0</td>\n",
       "        <td>1237.0</td>\n",
       "        <td>1394.0</td>\n",
       "        <td>1203.0</td>\n",
       "        <td>460.0</td>\n",
       "        <td>1039.0</td>\n",
       "        <td>1306.0</td>\n",
       "        <td>1347.0</td>\n",
       "        <td>1353.0</td>\n",
       "        <td>1072.0</td>\n",
       "        <td>1314.0</td>\n",
       "        <td>1302.0</td>\n",
       "        <td>1176.0</td>\n",
       "        <td>1422.0</td>\n",
       "        <td>1588.0</td>\n",
       "        <td>1315.0</td>\n",
       "        <td>1217.0</td>\n",
       "        <td>1154.0</td>\n",
       "        <td>1243.0</td>\n",
       "        <td>1024.0</td>\n",
       "        <td>1322.0</td>\n",
       "        <td>1204.0</td>\n",
       "        <td>1481.0</td>\n",
       "        <td>1293.0</td>\n",
       "        <td>1354.0</td>\n",
       "        <td>1223.0</td>\n",
       "        <td>1151.0</td>\n",
       "        <td>1136.0</td>\n",
       "        <td>1260.0</td>\n",
       "        <td>1154.0</td>\n",
       "        <td>914.0</td>\n",
       "        <td>1141.0</td>\n",
       "        <td>1459.0</td>\n",
       "        <td>1332.0</td>\n",
       "        <td>1213.0</td>\n",
       "        <td>1409.0</td>\n",
       "        <td>1211.0</td>\n",
       "        <td>1297.0</td>\n",
       "        <td>1281.0</td>\n",
       "        <td>1296.0</td>\n",
       "        <td>937.0</td>\n",
       "        <td>1297.0</td>\n",
       "        <td>1257.0</td>\n",
       "        <td>1139.0</td>\n",
       "    </tr>\n",
       "</table><br><b>Table 2 - Data for insurance costs across the united states per year, adjusted for inflation</b>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print results\n",
    "pt = PrettyTable()\n",
    "\n",
    "# Setup column names but skip DC and United states total\n",
    "pt.field_names = [\"Year\"] + [state for state in sorted(insurance_costs[2013].keys()) \\\n",
    "                             if \"district\" not in state and 'united' not in state]\n",
    "\n",
    "# Add rows from dict we have on insurance costs\n",
    "for year in insurance_costs.keys():\n",
    "    pt.add_row([year] + [insurance_costs[year][state] for state in sorted(insurance_costs[year].keys()) \\\n",
    "                         if \"district\" not in state and 'united' not in state])\n",
    "\n",
    "# Print as HTML\n",
    "HTML(pt.get_html_string() + caption(\"Data for insurance costs across the united states per year, adjusted for inflation\", 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Variable: Health Care Costs\n",
    "\n",
    "Calculated from the CEX data from above, essentially done once regional differencing is done\n",
    "\n",
    "### TODO\n",
    "\n",
    "* Complete data load once regional differences are figured out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Model Variable: Other Necessities Cost\n",
    "\n",
    "Calculated from the CEX data from above, essentially done once regional differencing is done\n",
    "\n",
    ">   Expenditures for other necessities are based on \n",
    "2013 data by household size  from  the  2014 Bureau  of  Labor  Statistics  Consumer  Expenditure  Survey\n",
    "including: (1) Apparel  and  services,  (2)  Housekeeping  supplies,  (3)  Personal  care  products  and  services, \n",
    "(4)  Reading, and (5) Miscellaneous.  These costs were further adjusted for regional differences using annual  expenditure  shares  reported  by  region\n",
    "\n",
    "\n",
    "\n",
    "### TODO\n",
    "\n",
    "* Complete data load once regional differences are figured out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff for region 0: 2134.921071 2096.000000\n",
      "Diff for region 1: 2399.604462 2127.000000\n",
      "Diff for region 2: 2129.205730 2253.000000\n",
      "Diff for region 3: 2245.958136 2284.000000\n"
     ]
    }
   ],
   "source": [
    "# Update cex dictionary with values for other variable\n",
    "cex[2013].update( \n",
    "    {\n",
    "        \"single_adults\": 37884.0,\n",
    "        \"other\": {\n",
    "            \"apparel\": 226385.0, \n",
    "            \"housekeeping\": 80097.0,\n",
    "            \"personal_care\": 81837.0,\n",
    "            \"reading\": 13086,\n",
    "            \"misc\": 99290,\n",
    "            \n",
    "            \"apparel_percent\": 0.13,\n",
    "            \"housekeeping_percent\": 0.164,\n",
    "            \"personal_care_percent\": 0.182,\n",
    "            \"reading_percent\": 0.205,\n",
    "            \"misc_percent\": 0.228,\n",
    "            \n",
    "            \"apparel_region\": [ x / 3.3 for x in (3.3, 3.6, 3.2, 3.3)],\n",
    "            \"housekeeping_region\": [ x / 1.2 for x in (1.0, 1.4, 1.2, 1.1 )],\n",
    "            \"personal_care_region\": [ x / 1.2 for x in (1.2, 1.3, 1.2, 1.2 )],\n",
    "            \"reading_region\": [ x / 1.0 for x in (1,1,1,1)],\n",
    "            \"misc_region\": [ x / 1.5 for x in (1.4, 1.5, 1.3, 1.6)],\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Values for 'other' from county webpages\n",
    "ideal_other_2013 = (2096, 2127, 2253, 2284)\n",
    "\n",
    "for region in range(4):\n",
    "    val = (1000000 * \n",
    "         ( (\n",
    "            (cex[2013][\"other\"][\"apparel\"] * cex[2013][\"other\"][\"apparel_percent\"] * cex[2013][\"other\"][\"apparel_region\"][region]) + \\\n",
    "            (cex[2013][\"other\"][\"housekeeping\"] * cex[2013][\"other\"][\"housekeeping_percent\"] * cex[2013][\"other\"][\"housekeeping_region\"][region]) + \\\n",
    "            (cex[2013][\"other\"][\"personal_care\"] * cex[2013][\"other\"][\"personal_care_percent\"] * cex[2013][\"other\"][\"personal_care_region\"][region]) + \\\n",
    "            (cex[2013][\"other\"][\"reading\"] * cex[2013][\"other\"][\"reading_percent\"] * cex[2013][\"other\"][\"reading_region\"][region]) + \\\n",
    "            (cex[2013][\"other\"][\"misc\"] * cex[2013][\"other\"][\"misc_percent\"] * cex[2013][\"other\"][\"misc_region\"][region])\n",
    "        ) /  (float(cex[2013][\"single_adults\"] * 1000)) ) * inflation_multipliers[2013])\n",
    "\n",
    "    # Print difference between calc and data from website\n",
    "    print \"Diff for region %d: %f %f\" % (region, val, ideal_other_2013[region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37884000.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(cex[2013][\"single_adults\"] * 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taxes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimum Wage or Mediun Wage per County or State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "-----\n",
    "\n",
    "-----\n",
    "\n",
    "## Creating Final Merged Data Frame\n",
    "\n",
    "Take all data loaded in prior into a multi-level index data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductory Analysis\n",
    "\n",
    "Create visualizations on:\n",
    "\n",
    "* Find national mean living wage gap, plot it over time\n",
    "* Look at distributions over states of living wage gap over time (facet grid, each graph is a state showing gap over time)\n",
    "* Seperate counties based on race and find national means of gap per year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlations with Economic Metrics\n",
    "\n",
    "* motion chart of states, x = gap, y = life exp, debt levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
